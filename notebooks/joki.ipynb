{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書分類ハンズオン\n",
    "\n",
    "## 機械学習モデル開発のワークフローと本章で扱う内容\n",
    "\n",
    "なお、動作確認は以下の環境で行いました。\n",
    "\n",
    "- Machine\n",
    "    - OS: Ubuntu 16.04\n",
    "    - CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
    "    - RAM: 64GB \n",
    "    - GPU: NVIDIA Tesla K80\n",
    "- Python\n",
    "    - Python 3.7\n",
    "    - PyTorch 1.3\n",
    "\n",
    "## 文書分類ハンズオン\n",
    "\n",
    "### 本章で扱うツールの解説\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "Transformersで用いることのできるモデルのリストはHugging Faceのホームページ (https://huggingface.co/models) にて公開されています。\n",
    "\n",
    "-  `bert-base-japanese`: Mecabで分かち書き & WordPieceでSubwordに分割して学習\n",
    "-  `bert-base-japanese-whole-word-masking`: 上記に加えて、whole word maskingを適用\n",
    "-  `bert-base-japanese-char`: Mecabで分かち書き & characterレベルのSubwordに分割して学習\n",
    "-  `bert-base-japanese-char-whole-word-masking`: 上記に加えて、whole word maskingを適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境によってインストールコマンドが異なります。https://pytorch.org/get-started/を参照してください。\n",
    "# 2019年12月現在、NVIDIAのGPUを搭載したLinuxマシンにAnacondaでPyTorchをインストールするコマンドは以下の通りです。\n",
    "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "# LinuxでかつGPUがない場合は !conda install pytorch torchvision cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (2.3.0)\n",
      "Collecting mecab-python3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/f3/6fe13dba17a7f81484b5e86e2db270617ad1cf8a7d2a9d1abf7fc5eea72b/mecab_python3-0.996.2-cp37-cp37m-manylinux1_x86_64.whl (15.9MB)\n",
      "\u001b[K     |████████████████████████████████| 15.9MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (1.17.4)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (1.10.45)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from transformers) (4.41.0)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from sacremoses->transformers) (1.13.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.45 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from boto3->transformers) (1.13.45)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.45->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ubuntu/miniconda3/envs/joki/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.45->boto3->transformers) (0.15.2)\n",
      "Installing collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-0.996.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bert-base-japanese*` は形態素解析のライブラリとして内部でMeCabを用いているので、`mecab-python3` もインストールしておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTを用いて日本語の文章を分類する手順はおおまかに書くと以下のようになります。\n",
    "\n",
    "1. aaa\n",
    "2. aaa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['いつも',\n",
       " 'プレゼンテーション',\n",
       " 'の',\n",
       " '撮影',\n",
       " 'に',\n",
       " '無',\n",
       " '##音',\n",
       " 'カメラ',\n",
       " '##アプリ',\n",
       " 'を',\n",
       " 'ご',\n",
       " '利用',\n",
       " 'いただ',\n",
       " '##き',\n",
       " 'ありがとう',\n",
       " 'ござい',\n",
       " 'ます',\n",
       " '。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "tokenizer.tokenize('いつもプレゼンテーションの撮影に無音カメラアプリをご利用いただきありがとうございます。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの準備\n",
    "#### livedoor ニュースコーパス\n",
    "\n",
    "今回は日本語における自然言語処理の試験用データセットとしてしばしば用いられる「livedoor ニュースコーパス」を用います。\n",
    "\n",
    "livedoorニュースはもともと株式会社ライブドアが運営するニュースサイトでしたが、株式会社ライブドアが旧ハンゲームジャパン株式会社であるNHN Japan株式会社に買収され、現在はNHN Japanが社名変更したLINE株式会社により運営されています。livedoorニュースの記事の一部には「クリエイティブ・コモンズライセンス『表示 – 改変禁止』」が適用されており、営利目的を含めて再配布可能となっています。該当するニュース記事を2012年9月上旬に株式会社ロンウイットが収集し、HTMLタグの除去などクリーニングを施した状態で公開しているのが「livedoor ニュースコーパス」です。\n",
    "\n",
    "livedoor ニュースコーパスは以下のリンクよりダウンロード可能です。\n",
    "\n",
    "https://www.rondhuit.com/download.html#ldcc\n",
    "\n",
    "オープンソースの全文検索システムApache Solrで扱いやすいようXML形式でニュースが格納されている `livedoor-news-data.tar.gz` と、シンプルに各々のニュースをテキストファイルとして扱っている `ldcc-20140209.tar.gz` が公開されています。\n",
    "\n",
    "今回は後者の `ldcc-20140209.tar.gz` をダウンロードしてください。`tar xzvf ldcc-20140209.tar.gz` などにより解凍すると `text` という名前のディレクトリが出てきます。以下のPythonスクリプトを実行するとコーパスのダウンロードと圧縮ファイルの解凍が行われ、カレントディレクトリに `text` ディレクトリが作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# dataディレクトリの作成\n",
    "#os.makedirs('data', exist_ok=True)\n",
    "\n",
    "url = 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
    "file_name = 'ldcc-20140209.tar.gz'\n",
    "\n",
    "# dataディレクトリへのlivedoor ニュースコーパスのダウンロードと解凍\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    # tar.gzファイルを読み込み\n",
    "    with tarfile.open(file_name) as tar:\n",
    "        tar.extractall()\n",
    "    # tar.gzファイルを消去\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text` ディレクトリの中身の構造は以下の通りです。\n",
    "\n",
    "```\n",
    "text\n",
    "├── CHANGES.txt\n",
    "├── README.txt\n",
    "├── dokujo-tsushin\n",
    "├── it-life-hack\n",
    "├── kaden-channel\n",
    "├── livedoor-homme\n",
    "├── movie-enter\n",
    "├── peachy\n",
    "├── smax\n",
    "├── sports-watch\n",
    "└── topic-news\n",
    "```\n",
    "\n",
    "`dokujo-tsushin` から `topic-news` はディレクトリであり、それぞれにニュース記事のテキストが格納されています。\n",
    "\n",
    "```\n",
    "text\n",
    "├── CHANGES.txt\n",
    "├── README.txt\n",
    "├── dokujo-tsushin\n",
    "│   ├── LICENSE.txt\n",
    "│   ├── dokujo-tsushin-4778030.txt\n",
    "│   ├── dokujo-tsushin-4778031.txt\n",
    "│   ├── dokujo-tsushin-4782522.txt\n",
    "...（以下略）\n",
    "```\n",
    "\n",
    "ニュース提供元は以下の9つです。記事の本文だけを見て、その記事がどのカテゴリに属しているのか（独女通信のニュースなのか、ITライフハックのニュースなのか、など）を判別する文書分類モデルを作成するのが本章の目的です。\n",
    "\n",
    "- 独女通信 (http://dokujo.jp/)\n",
    "- ITライフハック (http://news.livedoor.com/category/vender/223/)\n",
    "- 家電チャンネル (http://news.livedoor.com/category/vender/kadench/)\n",
    "- livedoor HOMME (http://news.livedoor.com/category/vender/homme/)\n",
    "- MOVIE ENTER (http://news.livedoor.com/category/vender/movie_enter/)\n",
    "- Peachy (http://news.livedoor.com/category/vender/ldgirls/)\n",
    "- エスマックス (http://news.livedoor.com/category/vender/smax/)\n",
    "- Sports Watch (http://news.livedoor.com/category/vender/208/)\n",
    "- トピックニュース (http://news.livedoor.com/category/vender/news/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ラベリング\n",
    "\n",
    "### 前処理\n",
    "\n",
    "#### 形態素解析\n",
    "#### ストップワード除去\n",
    "\n",
    "### 文書分類モデル\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "net = BertForSequenceClassification.from_pretrained('bert-base-japanese-whole-word-masking', num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価と比較\n",
    "\n",
    "### モデルのデプロイ\n",
    "\n",
    "## まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
