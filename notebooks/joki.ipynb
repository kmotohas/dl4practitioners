{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章 タイトル未定\n",
    "\n",
    "本章では文書分類モデルの作成を通じて自然言語処理で用いられる数々のPythonツールの利用方法を学びます。\n",
    "\n",
    "なお、動作確認は以下の環境で行いました。\n",
    "\n",
    "- Machine (AWS EC2 p2.xlargeインスタンス)\n",
    "    - OS: Ubuntu 16.04\n",
    "    - CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
    "    - RAM: 64GB \n",
    "    - GPU: NVIDIA Tesla K80\n",
    "- Python\n",
    "    - python: 3.7.5\n",
    "    - mecab-python3: 0.996.2\n",
    "    - torch (PyTorch): 1.3.1\n",
    "    - torchtext: 0.4.0\n",
    "    - transformers 2.3.0\n",
    "    - spaCy 2.2.3\n",
    "    - cupy 7.0.0\n",
    "    \n",
    "## 3.1 機械学習モデル開発のワークフローと本章で扱う内容\n",
    "\n",
    "文書分類のモデルは基本的に**教師あり学習**の枠組みで訓練します。つまり、図のようにテキストとラベルのペアで訓練データを用意します。例えばニュース記事があるとして、その記事はスポーツニュースなのか、芸能ニュースなのか、政治ニュースなのかを記事内の文章から分類したいとします。このときニュース記事の文章と、分類すべきカテゴリーの名前をペアとして扱います。\n",
    "\n",
    "![./figures/training.png](./figures/training.png)\n",
    "<center>図出典: https://spacy.io/usage/training</center>\n",
    "\n",
    "大まかにいうと大体以下の様な流れに沿って機械学習モデルの学習を進めていきます。\n",
    "\n",
    "1. 訓練用のデータセットを用意する\n",
    "2. 分類に用いる機械学習モデルを準備する\n",
    "3. モデルに訓練データのテキストを入力して予測値を得る\n",
    "4. モデルの予測と真のラベルを比較する\n",
    "5. 誤差を減らすようなモデルのパラメーター (重み) の更新値 (gradient) を計算する\n",
    "6. モデルのパラメーターを更新する\n",
    "7. 3から6を繰り返す\n",
    "\n",
    "具体的な手順に関しては次の節以降で見ていきましょう。\n",
    "\n",
    "## 3.2 文書分類ハンズオン\n",
    "\n",
    "ここではExplosion AIが開発する汎用的な自然言語処理ツールである **spaCy** と、Hugging Faceが開発するtransformerベースの現在最先端のモデルを簡単に利用するためのツールである **transformers** を扱ってニュース記事の分類モデルを作成します。\n",
    "\n",
    "### 3.2.1 本章で扱う主な自然言語処理ツールの解説\n",
    "\n",
    "#### 3.2.1.1 spaCy\n",
    "\n",
    "spaCy (https://spacy.io/) とはExplosion AIにより開発されている自然言語処理用のライブラリです。spaCyには事前学習済みの統計モデルと単語ベクトルが付属しており、50以上の言語の形態素解析（トークン化）がサポートされています。また、品詞タグ付け、依存関係解析、固有表現抽出、およびテキスト分類のための単語バッグや簡単な畳み込みニューラルネットワークモデルも備えています。MITライセンスの下でリリースされた商用のオープンソースソフトウェアです。\n",
    "\n",
    "spaCyのドキュメンテーション (https://spacy.io/usage/) に従ってインストールしてみましょう。環境によってインストールコマンドが異なるので適宜ドキュメンテーションを参考にしてください。ここではGPU付きのオプションでインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy[cuda]\n",
    "# pip install -U spacy  # GPUを利用しない場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyと同時に、Explosion AIにより開発されているThinc (https://github.com/explosion/thinc) という機械学習ライブラリや、Preferred NetworksのCUDA対応のNumPy互換行列計算ライブラリであるCuPy (https://cupy.chainer.org/) などがインストールされます。\n",
    "\n",
    "日本語の形態素解析ツールのMeCab (http://taku910.github.io/mecab/) もインストールしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書きのテストをしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('ja')\n",
    "for word in nlp('すもももももももものうち'):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うまく単語ごとに分割してくれていますね。本章でのスコープからは外れますが、リクルートと国立国語研究所が開発した GiNZAをインストールするとspaCy経由で日本語文章の解析処理を簡単に行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GiNZAのモデルを利用すると品詞タグ付けのみならず、トークン間の依存関係ラベリングや固有表現抽出などを行うことができます。ここではトークン間の依存関係を図示する例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "doc = nlp('すもももももももものうち')\n",
    "displacy.render(doc, style=\"dep\", options={\"compact\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GiNZAとspaCyに関してはオージス総研グループによる「はじめての自然言語処理　第4回 spaCy/GiNZA を用いた自然言語処理」 (https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part4.html) に詳しい解説がありますのでそちらを参照することをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.2 Transformers\n",
    "\n",
    "Transformersはtransformerベースの汎用アーキテクチャ （BERT、GPT-2、RoBERTa、XLM、DistilBert、XLNet、CTRL、...） を利用するためのオープンソースのシンプルなAPIを提供しています。開発はHugging Faceにより行われており、100以上の言語に対応した事前学習済みモデルが公開されています。ディープラーニングフレームワークとしてはGoogleのTensorFlow 2.0およびFaceBookが開発しているPyTorchに対応しています。ここではPyTorchを用いることにします。\n",
    "\n",
    "Transformersで用いることのできるモデルのリストはHugging Faceのホームページ (https://huggingface.co/models) にて公開されています。日本語用BERTモデルとしては、執筆時点 (2019年12月) では東北大学 乾・鈴木研究室が公開している以下の4つのモデルを利用可能です。\n",
    "\n",
    "-  `bert-base-japanese`:\n",
    "-  `bert-base-japanese-whole-word-masking`\n",
    "-  `bert-base-japanese-char`\n",
    "-  `bert-base-japanese-char-whole-word-masking`\n",
    "\n",
    "BERTには大きく分けて `BERT-Base` (12-layer, 768-hidden, 12-heads, 110M parameters) と `BERT-Large` (24-layer, 1024-hidden, 16-heads, 340M parameters) のふたつのアーキテクチャーがあります。上記のモデルは `BERT-Base` を日本語のWikipediaのデータを用いて訓練したものです。`bert-base-japanese` はMeCabとWordPieceと呼ばれる手法を用いてテキストの分かち書きを行った後に訓練されたものであり、`bert-base-japanese-char` ではテキストを文字ごとに分割しています。\n",
    "\n",
    "BERTの訓練時に行うタスクのひとつに、文章内のトークンをマスクし、そのトークンを予測する、というものがあります。例えば `\"He likes playing the piano.\"` という文章を `\"He likes [MASK] ##ing the piano.\"` という文章に変換し、`[MASK]` に含まれるトークンを予測します。なお、`##` から始まるトークンは接尾辞を表しています。このとき、単語に対応するトークンはまとめてマスクするのが Whole Word Masking と呼ばれる手法です。先ほどの文章を `\"He likes [MASK] [MASK] the piano.\"` のようにマスクして訓練したモデルが `bert-base-japanese-whole-word-masking` および、`bert-base-japanese-char-whole-word-masking` です。\n",
    "\n",
    "さて、transformersをインストールしてみましょう。環境によってインストールコマンドが異なります。https://pytorch.org/get-started/ を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019年12月現在、NVIDIAのGPUを搭載したLinuxマシンにAnacondaでPyTorchをインストールするコマンドは以下の通りです。\n",
    "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "# LinuxでかつGPUがない場合は conda install pytorch torchvision cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchで自然言語処理を行うときに便利なライブラリであるtorchtextもインストールしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install torchtext -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformersのインストールもpipを用いて簡単に行えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、以降で補助的に利用するライブラリもインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn seaborn mojimoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "- mojimoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformersの `BertJapaneseTokenizer` を用いた日本語文章の分かち書きのテストもしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "tokenizer.tokenize('いつもプレゼンテーションの撮影に無音カメラアプリをご利用いただきありがとうございます。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 データセットの準備\n",
    "\n",
    "次に機械学習モデルの訓練の要であるデータセットの準備をします。ここではlivedoor ニュースコーパスの各段落の文章を入力としてニュースサービスのカテゴリ名を判別するという問題設定でデータセットを作成します。\n",
    "\n",
    "#### 3.2.2.1 livedoor ニュースコーパス\n",
    "\n",
    "今回は日本語における自然言語処理の試験用データセットとしてしばしば用いられる「livedoor ニュースコーパス」を用います。\n",
    "\n",
    "livedoorニュースはもともと株式会社ライブドアが運営するニュースサイトでしたが、株式会社ライブドアが旧ハンゲームジャパン株式会社であるNHN Japan株式会社に買収され、現在はNHN Japanが社名変更したLINE株式会社により運営されています。livedoorニュースの記事の一部には「クリエイティブ・コモンズライセンス『表示 – 改変禁止』」が適用されており、営利目的を含めて再配布可能となっています。該当するニュース記事を2012年9月上旬に株式会社ロンウイットが収集し、HTMLタグの除去などクリーニングを施した状態で公開しているのが「livedoor ニュースコーパス」です。\n",
    "\n",
    "livedoor ニュースコーパスは以下のリンクよりダウンロード可能です。\n",
    "\n",
    "https://www.rondhuit.com/download.html#ldcc\n",
    "\n",
    "オープンソースの全文検索システムApache Solrで扱いやすいようXML形式でニュースが格納されている `livedoor-news-data.tar.gz` と、シンプルに各々のニュースをテキストファイルとして扱っている `ldcc-20140209.tar.gz` が公開されています。\n",
    "\n",
    "今回は後者の `ldcc-20140209.tar.gz` をダウンロードしてください。`tar xzvf ldcc-20140209.tar.gz` などにより解凍すると `text` という名前のディレクトリが出てきます。以下のPythonスクリプトを実行するとコーパスのダウンロードと圧縮ファイルの解凍が行われ、カレントディレクトリに `text` ディレクトリが作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# dataディレクトリの作成\n",
    "#os.makedirs('data', exist_ok=True)\n",
    "\n",
    "url = 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
    "file_name = 'ldcc-20140209.tar.gz'\n",
    "\n",
    "# dataディレクトリへのlivedoor ニュースコーパスのダウンロードと解凍\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    # tar.gzファイルを読み込み\n",
    "    with tarfile.open(file_name) as tar:\n",
    "        tar.extractall()\n",
    "    # tar.gzファイルを消去\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text` ディレクトリの中身の構造は以下の通りです。\n",
    "\n",
    "```\n",
    "text\n",
    "├── CHANGES.txt\n",
    "├── README.txt\n",
    "├── dokujo-tsushin\n",
    "├── it-life-hack\n",
    "├── kaden-channel\n",
    "├── livedoor-homme\n",
    "├── movie-enter\n",
    "├── peachy\n",
    "├── smax\n",
    "├── sports-watch\n",
    "└── topic-news\n",
    "```\n",
    "\n",
    "`dokujo-tsushin` から `topic-news` はディレクトリであり、それぞれにニュース記事のテキストが格納されています。先ほどのツリーの階層をひとつ深くしてみると以下の様になります。\n",
    "\n",
    "```\n",
    "text\n",
    "├── CHANGES.txt\n",
    "├── README.txt\n",
    "├── dokujo-tsushin\n",
    "│   ├── LICENSE.txt\n",
    "│   ├── dokujo-tsushin-4778030.txt\n",
    "│   ├── dokujo-tsushin-4778031.txt\n",
    "│   ├── dokujo-tsushin-4782522.txt\n",
    "...（以下略）\n",
    "```\n",
    "\n",
    "ニュース提供元は以下の9つです。記事の本文の各段落だけを見て、その記事がどのカテゴリに属しているのか（独女通信のニュースなのか、ITライフハックのニュースなのか、など）を判別する文書分類モデルを作成するのが本章の目的です。（記事の本文を全て一度に入力すると問題として簡単すぎるので、ここでは段落ごとに分割して独立した文章として扱っています。）\n",
    "\n",
    "- 独女通信 (http://news.livedoor.com/category/vender/90/)\n",
    "- ITライフハック (http://news.livedoor.com/category/vender/223/)\n",
    "- 家電チャンネル (http://news.livedoor.com/category/vender/kadench/)\n",
    "- livedoor HOMME (http://news.livedoor.com/category/vender/homme/)\n",
    "- MOVIE ENTER (http://news.livedoor.com/category/vender/movie_enter/)\n",
    "- Peachy (http://news.livedoor.com/category/vender/ldgirls/)\n",
    "- エスマックス (http://news.livedoor.com/category/vender/smax/)\n",
    "- Sports Watch (http://news.livedoor.com/category/vender/208/)\n",
    "- トピックニュース (http://news.livedoor.com/category/vender/news/)\n",
    "\n",
    "ちなみに、上記サービスのうちいくつかはドメインが変わっていたり終了しているので一部リンクが切れています。それぞれの記事ファイル（dokujo-tsushin-4778030.txtなど）は以下のフォーマットで構成されています。\n",
    "\n",
    "- １行目: 記事のURL\n",
    "- ２行目: 記事の日付\n",
    "- ３行目: 記事のタイトル\n",
    "- ４行目以降： 記事の本文\n",
    "\n",
    "このままでは少し扱いづらいのでひとつのtsv (tab-separated values) にまとめます。次のスクリプトの実行には多少時間がかかります。私の環境では6分弱かかりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# あまりに短い段落は除く\n",
    "minimum_sentence_length = 32\n",
    "\n",
    "# livedoor ニュースのサービス名のリスト\n",
    "services = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]\n",
    "# tsvファイルの各カラムのインデックス名のリスト\n",
    "index = ['url', 'datetime', 'title', 'body']\n",
    "\n",
    "# 空のPandasのDataFrameを準備\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# 各サービスのディレクトリでループ\n",
    "for service in services:\n",
    "    print('===== processing {} ====='.format(service))\n",
    "    # ニュース記事をすべて指定\n",
    "    # パスの例は './text/dokujo-tsushin/dokujo-tsushin-4778030.txt'\n",
    "    # LICENSE.txt は除外\n",
    "    wild_card = os.path.join('text', service, service + '*.txt')\n",
    "    file_paths = glob.glob(wild_card)\n",
    "    # 各ニュース記事のファイルパスでループ\n",
    "    for file_path in file_paths:\n",
    "        # ファイルを開いて一行ずつ読み込む\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # tsv のカラムを辞書型で用意\n",
    "            series_dict = {'service': service}\n",
    "            for num, line in enumerate(lines):\n",
    "                #line = line.replace('\\n', '')  # 改行を削除\n",
    "                # 0, 1, 2行目はそれぞれURL, 日付, 記事タイトルに相当\n",
    "                if num < len(index):\n",
    "                    series_dict[index[num]] = line\n",
    "                # 3行目以降は本文\n",
    "                elif line != '\\n' and line != '':\n",
    "                    series_dict['body'] += line\n",
    "                # lineが空（段落の境目もしくはファイルの末尾）の場合\n",
    "                else:\n",
    "                    if '関連記事' in series_dict['body']:\n",
    "                        pass\n",
    "                    elif '関連リンク' in series_dict['body']:\n",
    "                        pass\n",
    "                    # PandasのSeriesを作成し、DataFrameに追加していく\n",
    "                    elif len(series_dict['body']) > minimum_sentence_length:\n",
    "                        s = pd.Series(series_dict)\n",
    "                        df = df.append(s, ignore_index=True)\n",
    "                    # bodyを初期化\n",
    "                    series_dict['body'] = ''\n",
    "print('done')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成した `DataFrame` の最初の5行と最後の5行だけ抜き出して表示してみましょう。\n",
    "それぞれの行がニュース記事のひとつの段落に対応しています。ニュース記事自体は計7367個ありますが、32文字以下の短い段落を除くと全部で56631個の段落がありことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([df.head(3), df.tail(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各サービスごとの段落数を数えるには `pandas.Series.value_counts()` メソッドを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['service'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.Series.plot` を利用してグラフを作成することも可能です。ここでは `kind='bar'` で棒グラフを指定し、横軸のラベルを見やすい様に `rot=45` で45度回転し、y軸の範囲を `ylim=(0, 10000)` で0から10000に指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df['service'].value_counts()).plot(kind='bar', rot=45, ylim=(0, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的にラベルは文字列のままでは扱いづらい（ことが多い）のでscikit-learnの `LabelEncoder` (`sklearn.preprocessing.LabelEncoder`) を用いて整数値に変換してあげます。処理後は `'dokujo-tsushin'` が `0` になり、`it-life-hack` が `1` になり、`kaden-channel` が `2` …といった様に変換されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['service'] = le.fit_transform(df.service.values)\n",
    "pd.concat([df.head(3), df.tail(3)])  # 最初と最後の3行を抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`service` の列が文字列でなく非負の整数値になっていることに注目してください。\n",
    "ここで、scikit-learnの `train_test_split` (`sklearn.model_selection.train_test_split`) を用いてデータセットを「訓練データ」、「バリデーションデータ」、「テストデータ」に分割します。それぞれの役割は以下の通りです。\n",
    "\n",
    "- 訓練データ: 機械学習モデルの重みの更新に利用する学習データ\n",
    "- バリデーションデータ: 機械学習モデルの学習状況やハイパーパラメーターチューニングの良し悪しのチェックに用いるデータ\n",
    "- テストデータ: 最終的に機械学習モデルの性能評価に用いるデータ\n",
    "\n",
    "![holdout.png](./figures/holdout.png)\n",
    "\n",
    "実際の学習は訓練データのみを用います。他ふたつはあくまで性能評価のために用います。本章ではハイパーパラメーターチューニングは行いませんが、形式的にこの通り分割することにします。それぞれのデータ量の割合は適当に8:1:1としました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = df[['body', 'service']]  # 本文とラベルのみ抽出\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.2)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` をCSV (Comma-Separated Value) やTSV (Tab-Separated Value) で保存するには `pandas.DataFrame.to_csv` メソッドを呼び出します。ひとつ目の引数 `path_or_buf` には保存先のファイルパス（もしくはファイルオブジェクト）を指定し、ふたつ目の引数 `sep` には列のセパレーターを指定します。デフォルトでは `sep=','` となっており、セパレーターはカンマ、つまり `DataFrame` はCSVで保存されます。自然言語処理を行う場合、データ内にカンマが含まれていることがあるのでしばしばセパレーターとしてはタブ (`\\t`) が用いられます。ここでは `DataFrame` をTSVの形式で保存します。\n",
    "\n",
    "デフォルトでは `index` 引数は `True` になっています。そのままにするとTSVファイルに行番号の数値（この場合0から56630）がひとつのカラムとして追加されます。必要ないので `index=False` としておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.tsv', sep='\\t', index=False)\n",
    "val_df.to_csv('val.tsv', sep='\\t', index=False)\n",
    "test_df.to_csv('test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでデータセットを分割してTSVファイルとして保存できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 文書分類モデル\n",
    "\n",
    "ここでは単語バッグ、シンプルなCNN、そしてBERTをそれぞれ用いて文書分類を行ってみます。単語バッグとCNNのモデルはspaCyのAPIから利用できます。BERTはtransformersから利用します。訓練状況のモニタリングにはTensorBoardを用います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3.1 単語バッグ  (bag-of-words)\n",
    "\n",
    "単語バッグとはデータセットのテキスト中の単語のヒストグラムのことです。つまり、それぞれの単語の出現数を特徴量として扱います。例えば \"how does it feel like to wake up in the sun\", \"how does it feel like to shine on everyone\" というふたつの文があったとします。この中の単語 (トークン) それぞれに番号を付けると例えば以下の様になります。\n",
    "\n",
    "`{0: 'how', 1: 'does', 2: 'it', 3: 'feel', 4: 'like', 5: 'to', 6: 'wake', 7: 'up', 8: 'in', 9: 'the', 10: 'sun', 11: 'shine', 12: 'on', 13: 'everyone'}` \n",
    "\n",
    "それぞれの文を単語バッグとして表現すると以下の様になります。トークンの番号に対応するインデックスにそのトークンの出現数を詰めたリストとも言えます。\n",
    "\n",
    "`[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]`\n",
    "\n",
    "`[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]`\n",
    "\n",
    "単語バッグを用いればテキストを、データセットに出現するトークン数だけ次元がある（この場合は14次元の）ベクトルとみなすことができます。トークンひとつごとに番号を割り当てるのではなく、連続するトークンの対や三つ組など複数個をひとまとめに扱う手法を **n-グラム (n-gram)** と呼びます。spaCyの単語バッグはデフォルトでは `ngram_size=1` となっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('train.tsv', delimiter='\\t')\n",
    "df_val = pd.read_csv('val.tsv', delimiter='\\t')\n",
    "df_test = pd.read_csv('test.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train.body\n",
    "val_texts = df_val.body\n",
    "test_texts = df_test.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニュース文章を分かち書きします。私の環境ではそれぞれ30秒、4秒、3秒ほどかかりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()  # GPUがない場合、この行はスキップ\n",
    "nlp = spacy.blank('ja')\n",
    "\n",
    "train_docs = list(nlp.pipe(train_texts))\n",
    "val_docs = list(nlp.pipe(val_texts))\n",
    "test_docs = list(nlp.pipe(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyのAPIに合わせて正解ラベルを整形してあげましょう。訓練時に用いる `spacy.Language.update` メソッドはラベルデータを辞書型のリストとして受け取るのですが、キーに `'words', 'tags', 'heads', 'deps', 'entities', 'cats', 'links'` のどれかが存在することを前提としています。文書分類の場合は `'cats'` が必要です。猫ではなく、カテゴリーの略です。それぞれのデータに対するラベルは以下の様な構成であることが求められています。キー `'cats'` の値がまた辞書型で、分類すべきラベルのキーの値のみ `True` で他は `False` となります。\n",
    "\n",
    "```\n",
    "[\n",
    "\"\"\"データ#0\"\"\"\n",
    " {'cats': {'dokujo-tsushin': True,\n",
    "           'it-life-hack': False,\n",
    "           'kaden-channel': False,\n",
    "           'livedoor-homme': False,\n",
    "           'movie-enter': False,\n",
    "           'peachy': False,\n",
    "           'smax': False,\n",
    "           'sports-watch': False,\n",
    "           'topic-news': False}},\n",
    "\"\"\"データ#1\"\"\"\n",
    " {'cats': {'dokujo-tsushin': False,\n",
    "           'it-life-hack': True,\n",
    "           'kaden-channel': False,\n",
    "           'livedoor-homme': False,\n",
    "           'movie-enter': False,\n",
    "           'peachy': False,\n",
    "           'smax': False,\n",
    "           'sports-watch': False,\n",
    "           'topic-news': False}},\n",
    "\"\"\"データ#2以降\"\"\"\n",
    "...]\n",
    "```\n",
    "\n",
    "リスト内包表記と辞書内包表記を併用していてやや見づらいですが、以下の様にして `DataFrame` に対してループを回して整形します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]\n",
    "\n",
    "train_cats = [{'cats': {service: service == services[idx] for service in services}}\n",
    "              for idx in df_train.service]\n",
    "val_cats = [{'cats': {service: service == services[idx] for service in services}}\n",
    "            for idx in df_val.service]\n",
    "test_cats = [{'cats': {service: service == services[idx] for service in services}}\n",
    "             for idx in df_test.service]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんと期待通りに整形できているか確かめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cats[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題なければテキストとラベルのペアをリストに詰めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_docs, train_cats))\n",
    "val_data = list(zip(val_docs, val_cats))\n",
    "test_data = list(zip(test_docs, test_cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまででspaCyで文書分類を行うときのデータセットの準備は完了です。\n",
    "\n",
    "##### 文書分類モデルの訓練\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyには自然言語処理のシーケンスとしてパイプラインという概念があります。\n",
    "![pipeline.svg](./figures/pipeline.svg)\n",
    "<center>図出典: https://spacy.io/usage/processing-pipelines</center>\n",
    "なお、前述のGiNZAのモデルを用いるとトークナイザーによる分かち書きだけでなく、品詞タグ付け、固有表現抽出、依存関係解析などの処理がパイプラインに含まれます。ここでは `nlp = spacy.blank('ja')` としてトークナイザー (MeCab) の処理のみ持ったパイプラインを指定しています。以下でパイプラインに文書分類モデルを `textcat` という名前で追加します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat', \n",
    "                              config={'exclusive_classes': True, 'architecture': 'bow'})\n",
    "    nlp.add_pipe(textcat, last=True)  # パイプラインの末尾に追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'exclusive_classes': True` と指定するとモデルの出力層にソフトマックスが用いられます。つまり、モデルの出力がそれぞれのラベルに分類される確率として解釈できる様になります。なお、`'exclusive_classes': False` と指定すると出力層にはロジスティック関数が用いられます。排他的に分類するのではなく例えば、ニュース記事の分類時にこれはスポーツかつゴシップである、といった様に複数ラベルを割り当てたい時は後者を用います。`'architecture': 'bow'` で文書分類モデルの種類を単語バッグ (bag-of-words) に指定しています。\n",
    "\n",
    "次に、以下の様にしてモデルにラベルの情報を登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    textcat.add_label(service)\n",
    "    print(\"Add label %s.\" % (service))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いよいよ訓練を行いますが、その前に訓練状況を確認するための評価関数を用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(tokenizer, textcat, docs, cats, output_dict=False):\n",
    "    \"\"\"訓練時にモデルの性能を評価する関数\n",
    "    \n",
    "    引数:\n",
    "        tokenizer: パイプライン内に含まれるトークナイザー\n",
    "        textcat: 訓練対象の文書分類モデル\n",
    "        docs: 評価用データセットのテキストのリスト\n",
    "        cats: 評価用データセットのラベルのリスト\n",
    "              e.g. [{'cats': {'dokujo-tsushin': False, ...}}, {'cats': ...}]\n",
    "    返り値:\n",
    "        sklearn.metrics.classification_report: 各ラベルに対する性能の表\n",
    "            (str) if output_dict is False else (dict)\n",
    "    \"\"\"\n",
    "    # Trueのラベルの名前を y_true absのリストに詰める\n",
    "    # e.g. ['dokujo-tsushin', 'smax', 'peachy', ...]\n",
    "    y_true = [max(cat['cats'].items(), key=lambda x:x[1])[0] for cat in cats]\n",
    "    # 一番出力値が大きいラベルの名前を y_pred のリストに詰める\n",
    "    y_pred = []\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        prediction = max(doc.cats.items(), key=lambda x:x[1])[0]  # 予測のサービス名\n",
    "        y_pred.append(prediction)\n",
    "    return classification_report(y_true, y_pred, output_dict=output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、訓練を行いましょう。こちらのコードはspaCyのドキュメンテーション (https://spacy.io/usage/training#textcat) を参考にしています。大まかに以下の様な手順を踏んでいます。\n",
    "\n",
    "1. 訓練データに対してループを回す (ここではデータのミニバッチ化にspaCyの `minibatch` と `compunding` を用いる)\n",
    "2. `nlp.update` メソッドでモデルを更新する\n",
    "3. `evaluate`関数でモデルのパフォーマンスをチェックする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.util import minibatch, compounding\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs/bow/' + datetime.today().isoformat(timespec='seconds'))\n",
    "\n",
    "# 'textcat' 以外のパイプラインを抽出 (今回は空)\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "# エポック数（何回データセット全体に対してループを回すか）\n",
    "n_epochs = 32\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # textcat のみを訓練する\n",
    "    # >>> nlp.pipeline\n",
    "    # [('textcat', <spacy.pipeline.pipes.TextCategorizer object at 0x7f62703ee790>)]\n",
    "    textcat = nlp.pipeline[-1][-1]\n",
    "    optimizer = textcat.begin_training()\n",
    "    print('Training the model...')\n",
    "    # ミニバッチサイズのスケジューリング https://arxiv.org/abs/1711.00489\n",
    "    batch_sizes = compounding(4.0, 32.0, 1.001)  # サイズ4から始めて上限32まで1.001倍していく\n",
    "    num_samples = len(train_data)\n",
    "    for epoch in range(n_epochs):\n",
    "        print('===== iteration {}/{} ====='.format(epoch+1, n_epochs))\n",
    "        losses = {}\n",
    "        # 訓練データをシャッフルしてミニバッチ化する\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_sizes)  # generator\n",
    "        processed = 0\n",
    "        for batch in tqdm(batches):\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        # spaCyでは重みの移動平均をトラックしている\n",
    "        # モデルを利用する際は重みの最新値でなく平均値を利用\n",
    "        # cf. https://www.aclweb.org/anthology/P04-1015/\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            print('val loss = {:.4f}'.format(losses['textcat']))\n",
    "            # TensorBoard用のログ記録\n",
    "            writer.add_scalar('Loss vs Epoch/val', losses['textcat'], epoch+1)\n",
    "            # バリデーションデータに対してモデルを評価する\n",
    "            report = evaluate(nlp.tokenizer, textcat, val_docs, val_cats, output_dict=True)\n",
    "            print('val accuracy = {:.4f}'.format(report['accuracy']))\n",
    "            # TensorBoard用のログ記録\n",
    "            writer.add_scalar('Accuracy vs Epoch/val', report['accuracy'], epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別の端末で `$ tensorboard --logdir=./logs` のようにしてTensorBoardを立ち上げ、Google Chrome等のウェブブラウザで `http://localhost:6006` (ポート番号は `tensorboard` コマンドの出力を確認) にアクセスするとTensorBoardのUIを表示できます。これにより訓練状況のモニタリングを行うことができます。`ssh` 経由でリモートのサーバーで作業している場合は `ssh <user@host> -L6006:localhost:6006` などでポートフォワードしてあげてください。今回はバリデーションデータに対するエポックごとの損失関数の値 (Loss vs Epochs) と正解率 (Accuracy vs Epochs) をグラフ化しています。\n",
    "\n",
    "訓練が終わったらテストデータに対して性能評価をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = evaluate(nlp.tokenizer, textcat, test_docs, test_cats)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`accuracy` の行を見ると私の環境では0.82、つまり正解率が82%であることがわかります。\n",
    "\n",
    "訓練したモデルを保存するには `spacy.Language.to_disk` メソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.use_params(optimizer.averages):\n",
    "    nlp.to_disk('bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆に、保存したモデルを利用するには `spacy.load` メソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しにテストデータの中の、あえて一段落でなく、一文を分類してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ内の livedoor-homme のニュースの一文\n",
    "doc = nlp('転職者なら誰でも気になる採用する側の心理')\n",
    "print('true label is livedoor-home ||', doc.cats)\n",
    "# テストデータ内の smax のニュースの一段落\n",
    "doc = nlp('ARROWS X F-10Dが7月20日に発売')\n",
    "print('true label is smax ||', doc.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "きちんと正しいラベルに対応する出力値が大きくなっていますね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3.2 畳み込みニューラルネットワーク\n",
    "\n",
    "画像処理の分野で広く用いられている畳み込みニューラルネットワーク (Convolutional Neural Network、以下CNN) を自然言語処理のタスクに用いるのに違和感を感じる読者も多いかもしれません。しかし、テキストもトークンをベクトル化して並べたものと扱えば画像と同じく数値の行列としてみなせるのです。\n",
    "\n",
    "spaCyが提供する `simple_cnn` アーキテクチャーは大まかにいうと、トークンの埋め込み層 (embedding layer)、デフォルトでは4層の畳み込み層、ソフトマックスなどの分類層から構成されています。\n",
    "\n",
    "まずは先ほど利用した単語バッグモデルをパイプラインから削除しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('textcat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどと同様に `simple_cnn` モデルをパイプラインの末尾に配置し、ラベルを登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'textcat' not in nlp.pipe_names:\n",
    "    # 今回は 'architecture': 'simple_cnn'\n",
    "    textcat = nlp.create_pipe('textcat', config={'exclusive_classes': True, 'architecture': 'simple_cnn'})\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "\n",
    "for label in services:\n",
    "    textcat.add_label(label)\n",
    "    print('Add label %s.' % (label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとは単語バッグのときと全く同じ行程でCNNモデルの訓練を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.util import minibatch, compounding\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs/simple_cnn/' + datetime.today().isoformat(timespec='seconds'))\n",
    "\n",
    "# 'textcat' 以外のパイプラインを抽出 (今回は空)\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "# エポック数（何回データセット全体に対してループを回すか）\n",
    "n_epochs = 32\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # textcat のみを訓練する\n",
    "    # >>> nlp.pipeline\n",
    "    # [('textcat', <spacy.pipeline.pipes.TextCategorizer object at 0x7f62703ee790>)]\n",
    "    textcat = nlp.pipeline[-1][-1]\n",
    "    optimizer = textcat.begin_training()\n",
    "    print('Training the model...')\n",
    "    # ミニバッチサイズのスケジューリング https://arxiv.org/abs/1711.00489\n",
    "    batch_sizes = compounding(4.0, 32.0, 1.001)  # サイズ4から始めて上限32まで1.001倍していく\n",
    "    num_samples = len(train_data)\n",
    "    for epoch in range(n_epochs):\n",
    "        print('===== iteration {}/{} ====='.format(epoch+1, n_epochs))\n",
    "        losses = {}\n",
    "        # 訓練データをシャッフルしてミニバッチ化する\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_sizes)  # generator\n",
    "        processed = 0\n",
    "        for batch in tqdm(batches):\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        # spaCyでは重みの移動平均をトラックしている\n",
    "        # モデルを利用する際は重みの最新値でなく平均値を利用\n",
    "        # cf. https://www.aclweb.org/anthology/P04-1015/\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            print('val loss = {:.4f}'.format(losses['textcat']))\n",
    "            # TensorBoard用のログ記録\n",
    "            writer.add_scalar('Loss vs Epoch/val', losses['textcat'], epoch+1)\n",
    "            # バリデーションデータに対してモデルを評価する\n",
    "            report = evaluate(nlp.tokenizer, textcat, val_docs, val_cats, output_dict=True)\n",
    "            print('val accuracy = {:.4f}'.format(report['accuracy']))\n",
    "            # TensorBoard用のログ記録\n",
    "            writer.add_scalar('Accuracy vs Epoch/val', report['accuracy'], epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じくTensorBoardで訓練状況を確認しつつ、訓練が終わったらテストデータに対して性能評価をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(nlp.tokenizer, textcat, test_docs, test_cats)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私の環境では `accuracy` は0.84、つまり正解率84%でした。単語バッグのときは82%だったので2ポイントの改善が見られました。モデルを保存するには `spacy.Language.to_disk` メソッドを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.use_params(optimizer.averages):\n",
    "    nlp.to_disk('simple_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3.3 BERT\n",
    "\n",
    "ここからはspaCyではなくtransformersを用います。\n",
    "\n",
    "transformersはその名の通り **Transformer** (https://arxiv.org/abs/1706.03762) をベースにしたBERTなどの事前訓練済みモデルを簡単に利用するためのツールです。Transformer自体は \"Attention Is All You Need\" というセンセーショナルなタイトルの論文として2017年に発表されたGoogleのニューラルネットワークモデルです。BERTとは、ざっくり言うとTransformerを積み重ねたモデルです。BERTの論文 \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (https://arxiv.org/abs/1810.04805) は2018年にGoogleから発表されました。原論文においてはBERTをBookCorpus (https://github.com/soskek/bookcorpus) とWikipedia (https://www.wikipedia.org/) のデータを用いて事前訓練し、次の表の各タスクのデータセットに対してファインチューニングすることで当時最高 (State Of The Art) のスコアを叩き出したことで話題になりました。 \n",
    "\n",
    "|タスク|概要|前SOTA|BERT|\n",
    "|:----|:----|:----:|:----:|\n",
    "|GLUE|8種の言語理解タスク|75.2|81.9|\n",
    "|1. MNLI|2入力文の含意/矛盾/中立を判定|82.1|86.7|\n",
    "|2. QQP|2質問文が意味的に等価か判定|70.3|72.1|\n",
    "|3. QNLI|SQuADの改変．陳述文が質問文の解答を含むか判定|88.1|91.1|\n",
    "|4. SST-2|映画レビューの入力文のネガポジを判定|91.3|94.9|\n",
    "|5. CoLA|入力文が言語的に正しいか判定|45.4|60.5|\n",
    "|6. STS-B|ニュース見出しの2入力文の意味的類似性をスコア付け|80.0|86.5|\n",
    "|7. MRPC|ニュース記事の2入力文の意味的等価性を判定|82.3|89.3|\n",
    "|8. RTE|2入力文の含意を判定|56.0|70.1|\n",
    "|SQuAD|質疑応答タスク．陳述文から質問文の解答を抽出|91.7|93.2|\n",
    "|CoNLL|固有表現抽出タスク．単語に人物/組織/位置のタグ付け|92.6|92.8|\n",
    "|SWAG|入力文に後続する文を4つの候補文から選択|59.2|86.3|\n",
    "\n",
    "<center>表出典: http://deeplearning.hatenablog.com/entry/menhera_chan</center>\n",
    "\n",
    "あたかも画像処理の領域におけるImageNetデータセットで事前訓練したモデルのように、ある程度汎用的な特徴量抽出器として用いることができると期待されています。Wikipediaのような大きなデータセットを用いて訓練することで、入力のテキストを、目的のタスクを解くのに効率的な「表現」 (ベクトル) へと変換する役割をBERTないし多数提案されている進化版のモデルが担ってくれるのです。\n",
    "\n",
    "ただし、BERTの事前訓練は非常に高コストであり、原論文ではGoogleが開発しているTPU (Tensor Processing Unit) を16台用いて4日かかったと書いています。ですが、前述の通りありがたいことに東北大学 乾・鈴木研究室が日本語のWikipediaで事前訓練したモデル (https://github.com/cl-tohoku/bert-japanese) を公開しています。こちらのモデルはHugging Faceのリポジトリに登録されており、transformersから利用することができます。ここでは、`bert-base-japanese-whole-word-masking` と名付けられているモデルを利用してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "net = BertForSequenceClassification.from_pretrained('bert-base-japanese-whole-word-masking', num_labels=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数 `num_labels` を、livedoor ニュースコーパスのカテゴリー数である9に指定しています。こうすることで9カテゴリー分類のモデルとしてセットアップされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみにこのニューラルネットワークの構造をnetron (https://github.com/lutzroeder/netron) というツールを用いて可視化すると次のようになります。\n",
    "<img src=\"./figures/bert_classifier_netron.png\" alt=\"bert_classifier_netron\" width=\"150\">\n",
    "BERTモデルの構造が `BertModel` に押し込められているため、やけにシンプルに見えますが、ここではあまり深く考えないようにします。\n",
    "\n",
    "PyTorchを用いてディープラーニングを実装する際には、まず入力するデータとそのラベルなどをペアにして保持する `DataSet` というクラスと、そのデータに対してどのようにループを回すかを設定する `DataLoader` というクラスを作成します。\n",
    "\n",
    "ここでは、上記の作業を支援してくれるPyTorchの自然言語処理用パッケージの `torchtext` を使用します。なお、以降のコードはマイナビ出版の「つくりながら学ぶ！PyTorchによる発展ディープラーニング」を参考にさせていただいています。\n",
    "\n",
    "##### データの前処理\n",
    "\n",
    "最初に、分かち書き用のトークナイザーを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、日本語文章の前処理と分かち書きを合体させた関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'の', '年', '##収', 'は', '00', '万', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import mojimoji\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "        # 半角、全角の変換\n",
    "        text = mojimoji.han_to_zen(text)\n",
    "        # 改行、半角スペース、全角スペースを削除\n",
    "        text = re.sub('\\r', '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('　', '', text)\n",
    "        text = re.sub(' ', '', text)\n",
    "        # 数字文字の一律「0」化\n",
    "        text = re.sub(r'[0-9 ０-９]', '0', text)  # 数字\n",
    "        return tokenizer.tokenize(text)\n",
    "    \n",
    "# 動作確認\n",
    "text = '私の年収は53万です。'\n",
    "print(tokenizer_with_preprocessing(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように自然言語処理ではしばしば、解きたいタスクに数値の絶対値が関係しない場合、すべて0に置き換えるという前処理が行われます。\n",
    "\n",
    "`torchtext.data.Field` を利用すれば、データに対する前処理やその結果の管理をよしなにやってくれます。本文とラベルに対してそれぞれ `Field` オブジェクトを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "TEXT = Field(\n",
    "    sequential=True,                        # データの長さが可変か\n",
    "    tokenize=tokenizer_with_preprocessing,  # トークナイザーの関数の指定\n",
    "    use_vocab=True,                         # 語彙 (Vocabオブジェクト) を利用するか (後述)\n",
    "    lower=False,                            # アルファベットを大文字から小文字に変換するか\n",
    "    include_lengths=True,                   # 各データのトークン数のリストを保持するか\n",
    "    batch_first=True,                       # データのテンソルの0次元目をミニバッチの次元にするか\n",
    "    fix_length=512,                         # 全テキストの長さを固定するか (512はbert-japaneseの仕様)\n",
    "    init_token='[CLS]',                     # 文章の開始を表すトークン\n",
    "    eos_token='[SEP]',                      # 文章の終了を表すトークン\n",
    "    pad_token='[PAD]',                      # 長さ調整のパディングに用いるトークン\n",
    "    unk_token='[UNK]'                       # 未知語を表すトークン\n",
    ")\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込む際に行う処理を定義したらデータを実際に読み込みます。今回はデータをTSVファイルで保存していたので、`torchtext.data.TabularDataset` を利用します。`fields` 引数ではTSVファイルの各カラムの名前と `Field` オブジェクト (ここでは `TEXT` もしくは `LABEL`) のペアのタプルを列挙します。なお、`(name, None)` のようにすればそのカラムは無視されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path='.', train='train.tsv', validation='val.tsv', test='test.tsv', format='tsv', \n",
    "    fields=[('body', TEXT), ('service', LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  トークンの数値化\n",
    "\n",
    "テキストはそのままでは機械学習モデルから扱えないので、何かしらの方法で数値形式に変換する必要があります。単語バッグの例ではトークンごとにインデックスを振り、CNNの例では埋め込み表現 (ベクトル) として扱う方法を紹介しました。今回はBERTのトークナイザーに付随している語彙を利用します。日本語BERTのトークナイザーの語彙には `transformers.BertJapaneseTokenizer.vocab` でアクセスできます。型が `OrderedDict` なので一部のみ表示するにはややトリッキーな処理が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'の': 5, '、': 6, 'に': 7, '。': 8, 'は': 9, 'た': 10, 'を': 11, 'で': 12, 'と': 13, 'が': 14, 'し': 15, 'て': 16, '1': 17, 'な': 18, '年': 19}\n",
      "vocab size is 32000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "print(dict(itertools.islice(tokenizer.vocab.items(), 20)))\n",
    "print('vocab size is', len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この語彙を `TEXT` の `Field` の語彙にセットします。`stoi` は\"string to integer\"、つまり文字列と整数値の対応付けを表します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-e48969ceac47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEXT.build_vocab(train, min_freq=1)  # 訓練データに含まれるトークンから語彙を作成する場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# TEXT.build_vocab(train, min_freq=1)  # 訓練データに含まれるトークンから語彙を作成する場合\n",
    "TEXT.vocab.stoi = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_size = 32\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter = Iterator(train, batch_size, train=True, device=device)\n",
    "val_iter = Iterator(val, batch_size, train=False, sort=False, device=device)\n",
    "test_iter = Iterator(test, batch_size, train=False, sort=False, device=device)\n",
    "# あとで扱いやすいように辞書にまとめておく\n",
    "iterator_dict = {'train': train_iter, 'val': val_iter, 'test': test_iter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERTのファインチューニングのための設定\n",
    "\n",
    "理想的にはBERTのパラメーターすべて更新させていければよいのですが、それでは訓練にとても時間がかかってしまいます。また、すべてのパラメーターに対する勾配情報を保持しているとGPUのメモリも逼迫してしまいます。ここでは、BERT本体の最後のレイヤーおよび後段の分類層のパラメーターのみを更新する方針をとります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. すべてのパラメーターに対して勾配計算を False にする\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# 2. BertModel の最後のレイヤーのみ勾配計算 True にする\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "# 3. 分類器も勾配計算 True にする\n",
    "for name, param in net.classifier.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、モデルの最適化手法と損失関数を定義します。こちらの設定はBERTの原論文で用いられているパラメーターをそのまま採用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamアルゴリズムでそれぞれのパラメータを更新\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.classifier.parameters(), 'lr': 5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "# 損失関数の設定\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの訓練に用いる関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs/bert/' + datetime.today().isoformat(timespec='seconds'))\n",
    "\n",
    "def train_model(net, iterator_dict, criterion, optimizer, num_epochs):\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # ネットワークをGPUへ送る\n",
    "    net.to(device)\n",
    "    \n",
    "    # cuDNNのベンチマークモードをオンにして高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # ミニバッチサイズ\n",
    "    batch_size = iterator_dict['train'].batch_size\n",
    "    \n",
    "    # ログ用のイタレーション番号\n",
    "    iteration = 1\n",
    "    \n",
    "    # num_epochsだけループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練とバリデーションのループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードにする\n",
    "            else:\n",
    "                net.eval()   # モデルを評価モードにする\n",
    "                \n",
    "            epoch_loss = 0.     # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in iterator_dict[phase]:\n",
    "                inputs = batch.body[0]  # テキスト\n",
    "                labels = batch.service  # ラベル\n",
    "                \n",
    "                # optimizerの初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝搬計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 損失とネットワークの出力値を取得\n",
    "                    loss, outputs = net(input_ids=inputs, labels=labels)\n",
    "                    # ラベルを予測\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    # 訓練時は誤差逆伝搬\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10イタレーションに一度ログ出力\n",
    "                            # ミニバッチの正解率を計算\n",
    "                            acc = (torch.sum(preds == labels.data)).double() / batch_size\n",
    "                            print('iteration {} || Loss: {:.4f} || acc {}'.format(\n",
    "                                iteration, loss.item(), acc.item()))\n",
    "                            # 損失と正解率をTensorBoardに記録\n",
    "                            writer.add_scalar(\"Loss vs Iteration/{}\".format(phase), loss.item(), iteration)\n",
    "                            writer.add_scalar(\"Accuracy vs Iteration/{}\".format(phase), acc.item(), iteration)\n",
    "                        iteration += 1\n",
    "                    \n",
    "                    # epochの損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "        # epochごとの損失と正解率を計算\n",
    "        epoch_loss = epoch_loss / len(iterator_dict[phase].dataset)\n",
    "        epoch_acc = epoch_corrects.double() / len(iterator_dict[phase].dataset)\n",
    "        # TensorBoardに記録\n",
    "        writer.add_scalar(\"Loss vs Epoch/{}\".format(phase), epoch_loss, epoch + 1)\n",
    "        writer.add_scalar(\"Accuracy vs Epoch/{}\".format(phase), epoch_acc, epoch + 1)\n",
    "        \n",
    "        print('Epoch {}/{} | {} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            epoch + 1, num_epochs, phase, epoch_loss, epoch_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は24エポックの訓練およびバリデーションを実施します。私の環境では1エポック1時間くらいかかりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10 || Loss: 2.1883 || acc 0.0625\n",
      "iteration 20 || Loss: 2.1485 || acc 0.15625\n",
      "iteration 30 || Loss: 2.0787 || acc 0.1875\n",
      "iteration 40 || Loss: 2.2501 || acc 0.15625\n",
      "iteration 50 || Loss: 2.1193 || acc 0.25\n",
      "iteration 60 || Loss: 2.0386 || acc 0.25\n",
      "iteration 70 || Loss: 1.8626 || acc 0.28125\n",
      "iteration 80 || Loss: 2.0435 || acc 0.21875\n",
      "iteration 90 || Loss: 1.8986 || acc 0.28125\n",
      "iteration 100 || Loss: 1.9568 || acc 0.25\n",
      "iteration 110 || Loss: 1.8602 || acc 0.34375\n",
      "iteration 120 || Loss: 1.8653 || acc 0.46875\n",
      "iteration 130 || Loss: 1.7174 || acc 0.40625\n",
      "iteration 140 || Loss: 1.3343 || acc 0.53125\n",
      "iteration 150 || Loss: 1.5881 || acc 0.40625\n",
      "iteration 160 || Loss: 1.6485 || acc 0.46875\n",
      "iteration 170 || Loss: 1.4502 || acc 0.59375\n",
      "iteration 180 || Loss: 1.2198 || acc 0.5625\n",
      "iteration 190 || Loss: 1.5703 || acc 0.46875\n",
      "iteration 200 || Loss: 1.1808 || acc 0.625\n",
      "iteration 210 || Loss: 0.9294 || acc 0.71875\n",
      "iteration 220 || Loss: 1.4784 || acc 0.5\n",
      "iteration 230 || Loss: 1.2685 || acc 0.59375\n",
      "iteration 240 || Loss: 1.2424 || acc 0.6875\n",
      "iteration 250 || Loss: 1.3235 || acc 0.46875\n",
      "iteration 260 || Loss: 1.2150 || acc 0.5\n",
      "iteration 270 || Loss: 0.9215 || acc 0.6875\n",
      "iteration 280 || Loss: 1.0034 || acc 0.65625\n",
      "iteration 290 || Loss: 1.1570 || acc 0.59375\n",
      "iteration 300 || Loss: 1.1517 || acc 0.53125\n",
      "iteration 310 || Loss: 1.1261 || acc 0.65625\n",
      "iteration 320 || Loss: 1.3457 || acc 0.4375\n",
      "iteration 330 || Loss: 1.0587 || acc 0.625\n",
      "iteration 340 || Loss: 1.1150 || acc 0.625\n",
      "iteration 350 || Loss: 1.4252 || acc 0.4375\n",
      "iteration 360 || Loss: 1.4099 || acc 0.5\n",
      "iteration 370 || Loss: 1.1875 || acc 0.625\n",
      "iteration 380 || Loss: 1.5120 || acc 0.46875\n",
      "iteration 390 || Loss: 1.3841 || acc 0.5\n",
      "iteration 400 || Loss: 1.0042 || acc 0.65625\n",
      "iteration 410 || Loss: 1.0346 || acc 0.5625\n",
      "iteration 420 || Loss: 1.5028 || acc 0.5625\n",
      "iteration 430 || Loss: 0.9296 || acc 0.71875\n",
      "iteration 440 || Loss: 1.0111 || acc 0.65625\n",
      "iteration 450 || Loss: 1.0941 || acc 0.59375\n",
      "iteration 460 || Loss: 0.9309 || acc 0.5625\n",
      "iteration 470 || Loss: 0.9410 || acc 0.75\n",
      "iteration 480 || Loss: 0.9393 || acc 0.625\n",
      "iteration 490 || Loss: 1.1398 || acc 0.625\n",
      "iteration 500 || Loss: 1.2339 || acc 0.5625\n",
      "iteration 510 || Loss: 0.9052 || acc 0.8125\n",
      "iteration 520 || Loss: 1.2633 || acc 0.5625\n",
      "iteration 530 || Loss: 1.0507 || acc 0.6875\n",
      "iteration 540 || Loss: 0.8119 || acc 0.71875\n",
      "iteration 550 || Loss: 1.1376 || acc 0.53125\n",
      "iteration 560 || Loss: 1.2112 || acc 0.5625\n",
      "iteration 570 || Loss: 1.2021 || acc 0.59375\n",
      "iteration 580 || Loss: 1.0694 || acc 0.65625\n",
      "iteration 590 || Loss: 1.2249 || acc 0.4375\n",
      "iteration 600 || Loss: 1.0645 || acc 0.59375\n",
      "iteration 610 || Loss: 0.9918 || acc 0.59375\n",
      "iteration 620 || Loss: 0.8215 || acc 0.6875\n",
      "iteration 630 || Loss: 0.9340 || acc 0.625\n",
      "iteration 640 || Loss: 0.9285 || acc 0.6875\n",
      "iteration 650 || Loss: 1.1606 || acc 0.625\n",
      "iteration 660 || Loss: 1.0573 || acc 0.65625\n",
      "iteration 670 || Loss: 0.8963 || acc 0.6875\n",
      "iteration 680 || Loss: 1.4077 || acc 0.5\n",
      "iteration 690 || Loss: 1.1423 || acc 0.59375\n",
      "iteration 700 || Loss: 0.8724 || acc 0.75\n",
      "iteration 710 || Loss: 1.1477 || acc 0.625\n",
      "iteration 720 || Loss: 0.9465 || acc 0.625\n",
      "iteration 730 || Loss: 1.1514 || acc 0.625\n",
      "iteration 740 || Loss: 0.9526 || acc 0.71875\n",
      "iteration 750 || Loss: 0.7424 || acc 0.8125\n",
      "iteration 760 || Loss: 0.9298 || acc 0.65625\n",
      "iteration 770 || Loss: 1.1941 || acc 0.65625\n",
      "iteration 780 || Loss: 1.0390 || acc 0.625\n",
      "iteration 790 || Loss: 0.9820 || acc 0.71875\n",
      "iteration 800 || Loss: 1.2492 || acc 0.5\n",
      "iteration 810 || Loss: 1.0277 || acc 0.6875\n",
      "iteration 820 || Loss: 0.6385 || acc 0.75\n",
      "iteration 830 || Loss: 0.9549 || acc 0.625\n",
      "iteration 840 || Loss: 1.2928 || acc 0.53125\n",
      "iteration 850 || Loss: 0.7576 || acc 0.78125\n",
      "iteration 860 || Loss: 0.7965 || acc 0.71875\n",
      "iteration 870 || Loss: 0.9392 || acc 0.71875\n",
      "iteration 880 || Loss: 1.1222 || acc 0.5625\n",
      "iteration 890 || Loss: 0.8305 || acc 0.6875\n",
      "iteration 900 || Loss: 1.2763 || acc 0.53125\n",
      "iteration 910 || Loss: 0.8532 || acc 0.75\n",
      "iteration 920 || Loss: 0.8555 || acc 0.78125\n",
      "iteration 930 || Loss: 0.8910 || acc 0.6875\n",
      "iteration 940 || Loss: 0.8654 || acc 0.75\n",
      "iteration 950 || Loss: 0.6384 || acc 0.8125\n",
      "iteration 960 || Loss: 0.8187 || acc 0.65625\n",
      "iteration 970 || Loss: 1.0002 || acc 0.6875\n",
      "iteration 980 || Loss: 1.3175 || acc 0.40625\n",
      "iteration 990 || Loss: 1.0319 || acc 0.59375\n",
      "iteration 1000 || Loss: 0.8095 || acc 0.78125\n",
      "iteration 1010 || Loss: 0.9934 || acc 0.65625\n",
      "iteration 1020 || Loss: 1.1023 || acc 0.59375\n",
      "iteration 1030 || Loss: 1.3010 || acc 0.5625\n",
      "iteration 1040 || Loss: 1.0365 || acc 0.625\n",
      "iteration 1050 || Loss: 1.0864 || acc 0.65625\n",
      "iteration 1060 || Loss: 0.9033 || acc 0.6875\n",
      "iteration 1070 || Loss: 1.3652 || acc 0.5\n",
      "iteration 1080 || Loss: 0.9895 || acc 0.59375\n",
      "iteration 1090 || Loss: 0.8535 || acc 0.78125\n",
      "iteration 1100 || Loss: 0.7850 || acc 0.71875\n",
      "iteration 1110 || Loss: 1.1272 || acc 0.65625\n",
      "iteration 1120 || Loss: 1.0694 || acc 0.625\n",
      "iteration 1130 || Loss: 0.7718 || acc 0.6875\n",
      "iteration 1140 || Loss: 0.9653 || acc 0.71875\n",
      "iteration 1150 || Loss: 0.8891 || acc 0.71875\n",
      "iteration 1160 || Loss: 1.1830 || acc 0.53125\n",
      "iteration 1170 || Loss: 1.2196 || acc 0.65625\n",
      "iteration 1180 || Loss: 0.9102 || acc 0.6875\n",
      "iteration 1190 || Loss: 0.9797 || acc 0.71875\n",
      "iteration 1200 || Loss: 0.6838 || acc 0.8125\n",
      "iteration 1210 || Loss: 0.6686 || acc 0.8125\n",
      "iteration 1220 || Loss: 0.5884 || acc 0.8125\n",
      "iteration 1230 || Loss: 0.8534 || acc 0.71875\n",
      "iteration 1240 || Loss: 1.1452 || acc 0.625\n",
      "iteration 1250 || Loss: 1.2366 || acc 0.625\n",
      "iteration 1260 || Loss: 0.8194 || acc 0.71875\n",
      "iteration 1270 || Loss: 0.8616 || acc 0.65625\n",
      "iteration 1280 || Loss: 1.0556 || acc 0.65625\n",
      "iteration 1290 || Loss: 0.8208 || acc 0.71875\n",
      "iteration 1300 || Loss: 0.9234 || acc 0.59375\n",
      "iteration 1310 || Loss: 0.8772 || acc 0.75\n",
      "iteration 1320 || Loss: 0.7303 || acc 0.75\n",
      "iteration 1330 || Loss: 0.8337 || acc 0.71875\n",
      "iteration 1340 || Loss: 0.8553 || acc 0.75\n",
      "iteration 1350 || Loss: 1.1667 || acc 0.625\n",
      "iteration 1360 || Loss: 1.0863 || acc 0.5625\n",
      "iteration 1370 || Loss: 0.9396 || acc 0.65625\n",
      "iteration 1380 || Loss: 0.7320 || acc 0.78125\n",
      "iteration 1390 || Loss: 0.9763 || acc 0.71875\n",
      "iteration 1400 || Loss: 1.2530 || acc 0.53125\n",
      "iteration 1410 || Loss: 1.0735 || acc 0.6875\n",
      "Epoch 1/24 | val | Loss: 0.8513 Acc: 0.7048\n",
      "iteration 1420 || Loss: 0.5702 || acc 0.78125\n",
      "iteration 1430 || Loss: 1.0930 || acc 0.625\n",
      "iteration 1440 || Loss: 0.8646 || acc 0.71875\n",
      "iteration 1450 || Loss: 0.7275 || acc 0.75\n",
      "iteration 1460 || Loss: 0.6820 || acc 0.8125\n",
      "iteration 1470 || Loss: 0.6869 || acc 0.6875\n",
      "iteration 1480 || Loss: 0.6103 || acc 0.71875\n",
      "iteration 1490 || Loss: 0.7839 || acc 0.75\n",
      "iteration 1500 || Loss: 1.0972 || acc 0.625\n",
      "iteration 1510 || Loss: 0.8481 || acc 0.78125\n",
      "iteration 1520 || Loss: 0.6609 || acc 0.75\n",
      "iteration 1530 || Loss: 0.7066 || acc 0.75\n",
      "iteration 1540 || Loss: 0.6962 || acc 0.71875\n",
      "iteration 1550 || Loss: 0.7286 || acc 0.75\n",
      "iteration 1560 || Loss: 1.0084 || acc 0.59375\n",
      "iteration 1570 || Loss: 0.7120 || acc 0.75\n",
      "iteration 1580 || Loss: 1.0021 || acc 0.65625\n",
      "iteration 1590 || Loss: 0.6632 || acc 0.75\n",
      "iteration 1600 || Loss: 0.8065 || acc 0.71875\n",
      "iteration 1610 || Loss: 0.8898 || acc 0.78125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 24\n",
    "net_trained = train_model(net, iterator_dict, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、訓練したネットワークのモデルの保存と、テストデータでの正解率を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練したネットワークパラメーターを保存\n",
    "save_path = 'bert_trained.pth'\n",
    "torch.save(net_trained.state_dics(), save_path)\n",
    "# GPUを使えるかどうか\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net_trained.eval()      # モデルを評価モードにする\n",
    "net_trained.to(device)  # GPUが使えるならGPUに送る\n",
    "\n",
    "# 正解数\n",
    "num_corrects = 0\n",
    "\n",
    "# テストデータに対するループ\n",
    "for batch in tqdm(iterator_dict['test']):\n",
    "    inputs = batch.body[0]  # テキスト\n",
    "    labels = batch.service  # ラベル\n",
    "    \n",
    "    # 順伝搬計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "        _, outputs = net_trained(input_ids=inputs, labels=labels)\n",
    "        # ラベルを予測\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        # 正解数の合計を更新\n",
    "        num_corrects += torch.sum(preds == labels.data)\n",
    "# 正解率を計算\n",
    "acc = num_corrects.double() / len(iterator_dict['test'].dataset)\n",
    "print('テストデータ {} 個での正解率: {.4f}'.format(len(iterator_dict['test'].dataset), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率は86%程度となりました。単語バッグが82%、CNNが84%だったので、また少し上昇していることが確認できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価と比較\n",
    "\n",
    "spaCyの単語バッグ (bow) とCNN (simple_cnn)、そしてtransformersのBERTを利用してlivedoor ニュースコーパスの段落テキストからニュースサービス名を予測するモデルを訓練してきました。テストデータに対するそれぞれのモデルの正解率をまとめると次の図のようになります。\n",
    "\n",
    "<img src=\"./figures/accuracy_bow_cnn_bert.png\" alt=\"accuracy_bow_cnn_bert\" width=\"480\">\n",
    "\n",
    "今回の結果ではBERTの正解率が一番高いという結果になりました。ただし、単語バッグやCNNが1分弱で１エポック訓練できたのに対し、BERTは1時間程度かかっていました。また、仮にシステムにこのモデルを組み込む場合は、モデルの予測に用いるインフラの費用や予測にかかる時間も忘れてはいけません。そういった広い意味でのコストと精度とのトレードオフを考慮しながらどのモデルを選択するか考えましょう。いきなりSOTAのモデルから試してみるのではなく、まず最初は単純なベースラインモデルから実験してみるのをお勧めします。\n",
    "\n",
    "また、この結果だけを見て「BERTを利用しても大したゲインはない」と結論づけてはいけません。BERTなどの事前訓練モデルによるゲインはタスク依存性がとても強いです。基本的に文書分類のタスクというのは簡単な部類の問題です。人間がニュースの分類をするとき、なんとなく文章に含まれている単語を見ればパッと見で予測できますよね。そのため、単純な単語バッグのモデルでもある程度高い性能が達成されたのです。\n",
    "\n",
    "先に掲載したBERTとそれ以前のSOTAの比較表を再掲してみましょう。\n",
    "\n",
    "|タスク|概要|前SOTA|BERT|\n",
    "|:----|:----|:----:|:----:|\n",
    "|GLUE|8種の言語理解タスク|75.2|81.9|\n",
    "|1. MNLI|2入力文の含意/矛盾/中立を判定|82.1|86.7|\n",
    "|2. QQP|2質問文が意味的に等価か判定|70.3|72.1|\n",
    "|3. QNLI|SQuADの改変．陳述文が質問文の解答を含むか判定|88.1|91.1|\n",
    "|4. SST-2|映画レビューの入力文のネガポジを判定|91.3|94.9|\n",
    "|5. CoLA|入力文が言語的に正しいか判定|45.4|60.5|\n",
    "|6. STS-B|ニュース見出しの2入力文の意味的類似性をスコア付け|80.0|86.5|\n",
    "|7. MRPC|ニュース記事の2入力文の意味的等価性を判定|82.3|89.3|\n",
    "|8. RTE|2入力文の含意を判定|56.0|70.1|\n",
    "|SQuAD|質疑応答タスク．陳述文から質問文の解答を抽出|91.7|93.2|\n",
    "|CoNLL|固有表現抽出タスク．単語に人物/組織/位置のタグ付け|92.6|92.8|\n",
    "|SWAG|入力文に後続する文を4つの候補文から選択|59.2|86.3|\n",
    "\n",
    "<center>表出典: http://deeplearning.hatenablog.com/entry/menhera_chan</center>\n",
    "\n",
    "映画レビューの入力文のネガポジを判定するSST-2という文書分類タスクでは、当初のSOTAが91.3、BERTのモデルでは94.9というスコアでした。もともと高かったスコアがBERTによりさらに少し改善したという、今回の実験と似たような結果になっています。\n",
    "\n",
    "しかし、例えば表の一番下のSWAGというタスクに注目してみましょう。当初のSOTAが59.2でBERTでは86.3と、圧倒的に更新されていることがわかります。SWAGとは入力文に続く文を4つの候補分から選択する、TOEICテストの問題のようなタスクです。SWAGデータセットからひとつサンプルを抜き出してみます。もともとは英語の文章ですが、日本語訳は私が足しました。\n",
    "\n",
    "```\n",
    "On stage, a woman takes a seat at the piano. She \n",
    "(ステージ上で女性がピアノの椅子に座っています。彼女は)\n",
    "    a) sits on a bench as her sister plays with the doll. \n",
    "       (ベンチに座っている間、彼女の妹は人形で遊んでいます。)\n",
    "    b) smiles with someone as the music plays. \n",
    "       (音楽が流れている間、誰かと微笑んでいます。)\n",
    "    c) is in the crowd, watching the dancers. \n",
    "       (混雑の中、ダンサーを見ています。)\n",
    "    d) nervously sets her fingers on the keys.\n",
    "       (緊張しながら鍵盤に指を置きます。)\n",
    "```\n",
    "\n",
    "こちらは単純な文書分類とは異なり、ある程度文章の文脈を把握しないと解けない問題となっています。なお、正解は (d) です。こういった人間でも少し考えないと難しいようなタスクにはBERTが有効であると期待されます。ちなみに、SWAGのタスクを解きたい場合は `transformers.BertForMultipleChoice` クラスを用いるとよいでしょう。\n",
    "\n",
    "## まとめ\n",
    "\n",
    "本章ではlivedoor ニュースコーパスに対して単語バッグ、CNN、そしてBERTを用いて段落のカテゴリー分類モデルを作成しました。文書分類モデルは、カテゴリーが明確に分類可能なものとして定義されていれば一般的にさほど難しくないタスクですので、単語バッグやCNNなどの単純なモデルでも高い予測精度を達成できました。僅かながらBERTがこの中では最高の精度となりましたので、リソースに余裕があるのならばBERTを試してみるのもよいかもしれません。文脈の把握が必要となるようなより難しいタスクであればBERTを試してみるのが合理的でしょう。\n",
    "\n",
    "また、本章では深く触れませんでしたが、BERT系列のモデルとして現時点でもXLNetやRoBERTaなど、日々多数のモデルが提案されており、研究の発展のスピードがとても早いです。ぜひ、ここで紹介したモデル以外にも目を向けてみたり、最新のニュースにキャッチアップし、自然言語処理技術の発展を楽しみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [(Part 1) tensorflow2でhuggingfaceのtransformersを使ってBERTを文書分類モデルに転移学習する](https://tksmml.hatenablog.com/entry/2019/10/22/215000)\n",
    "- [(Part 2) tensorflow 2 でhugging faceのtransformers公式のBERT日本語学習済みモデルを文書分類モデルにfine-tuningする](https://tksmml.hatenablog.com/entry/2019/12/15/090900)\n",
    "- [All Models and checkpoints](https://huggingface.co/models)\n",
    "- [Working with GPU packages](https://docs.anaconda.com/anaconda/user-guide/tasks/gpu-packages/)\n",
    "- [gensimとPyTorchを使ったlive doorニュースコーパスのテキスト分類](https://www.pytry3g.com/entry/2018/04/03/194202)\n",
    "- [bert-japanese](https://github.com/cl-tohoku/bert-japanese)\n",
    "- [DocumentClassificationUsingBERT-Japanese](https://github.com/nekoumei/DocumentClassificationUsingBERT-Japanese)\n",
    "- [torchtext](https://torchtext.readthedocs.io/en/latest/index.html)\n",
    "- [FX予測 : PyTorchのBERTで経済ニュース解析](https://qiita.com/THERE2/items/8b7c94787911fad8daa6)\n",
    "- [torchtextで簡単にDeepな自然言語処理](https://qiita.com/itok_msi/items/1f3746f7e89a19dafac5)\n",
    "- [transformers](https://github.com/huggingface/transformers)\n",
    "- [BERTを使った文章要約 [身内向け]](https://qiita.com/IwasakiYuuki/items/25f5bbcde4f82dff7f1a)\n",
    "- [MeCab + Gensim による日本語の自然言語処理](https://www.koi.mashykom.com/nlp.html)\n",
    "- [論文解説 Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (GNMT)](http://deeplearning.hatenablog.com/entry/gnmt)\n",
    "- [BERT with SentencePiece で日本語専用の pre-trained モデルを学習し、それを基にタスクを解く](https://techlife.cookpad.com/entry/2018/12/04/093000)\n",
    "- [はじめての自然言語処理](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
